---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. Suspendisse condimentum, libero vel tempus mattis, risus risus vulputate libero, elementum fermentum mi neque vel nisl. Maecenas facilisis maximus dignissim. Curabitur mattis vulputate dui, tincidunt varius libero luctus eu. Mauris mauris nulla, scelerisque eget massa id, tincidunt congue felis. Sed convallis tempor ipsum rhoncus viverra. Pellentesque nulla orci, accumsan volutpat fringilla vitae, maximus sit amet tortor. Aliquam ultricies odio ut volutpat scelerisque. Donec nisl nisl, porttitor vitae pharetra quis, fringilla sed mi. Fusce pretium dolor ut aliquam consequat. Cras volutpat, tellus accumsan mattis molestie, nisl lacus tempus massa, nec malesuada tortor leo vel quam. Aliquam vel ex consectetur, vehicula leo nec, efficitur eros. Donec convallis non urna quis feugiat.

My research interests include computer vision techniques—particularly image and video segmentation—and agentic AI systems, with a focus on their applications in medical domains such as radiology, surgery, and pathology. I have published more than 10 papers at the top conferences and journals with total <a href='https://scholar.google.com/citations?user=fTnoBDAAAAAJ'>google scholar citations <strong><span id='total_cit'>700+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=fTnoBDAAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>).


# 🔥 News
- *2025.05*: &nbsp;🎉🎉 Our paper "ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible Long-term Tracking" has been early accepted by MICCAI 2025 (top 9%). 
- *2025.04*: &nbsp;🎉🎉 Our paper "The Four Color Theorem for Cell Instance Segmentation" has been accepted by ICML 2025. 
- *2025.03*: &nbsp;🎉🎉 Our paper "Seine: Structure encoding and interaction network for nuclei instance segmentation" has been accepted by IEEE JBHI. 
- *2025.02*: &nbsp;🎉🎉 Our paper "Medical sam adapter: Adapting segment anything model for medical image segmentation" has been accepted by MedIA. 
- *2024.12*: &nbsp;🎉🎉 Our paper "Multi-scale Context Intertwining for Panoramic Renal Pathology Segmentation" has been accepted by ICASSP 2025. 
- *2024.12*: &nbsp;🎉🎉 Our paper "Dawn: Domain-adaptive weakly supervised nuclei segmentation via cross-task interactions" has been accepted by IEEE TCSVT. 
- *2024.5*: &nbsp;🎉🎉 Our paper "Dynamic pseudo label optimization in point-supervised nuclei segmentation" has been early accepted by MICCAI 2024 (top11%).
- *2024.2*: &nbsp;🎉🎉 Our paper "Boundary-aware Contrastive Learning for Semi-supervised Nuclei Instance Segmentation" has been accepted by MIDL 2024 (Oral).


# 📝 Selected Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Under Review</div><img src='images/papers/map.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Medagent-pro: Towards evidence-based multi-modal medical diagnosis via reasoning agentic workflow](https://arxiv.org/pdf/2503.18968)

**Ziyue Wang**, Junde Wu, Linghan Cai, Chang Han Low, Xihong Yang, Qiaxuan Li, Yueming Jin

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Under Review</div><img src='images/papers/surgraw.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Surgraw: Multi-agent workflow with chain-of-thought reasoning for surgical intelligence](https://arxiv.org/pdf/2503.10265)

Chang Han Low, **Ziyue Wang**, Tianyi Zhang, Zhitao Zeng, Zhuo Zhu, Evangelos B. Mazomenos, Yueming Jin
<!-- [**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->
</div>
</div>

- [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020**

# 🎖 Honors and Awards
- *2023.10* Second-class academic schorlarship of undergraduate (top 10%).
- *2022.10* Second-class academic schorlarship of undergraduate (top 10%).
- *2022.10* 11th of the NeurIPS22 Cellseg challenge among 500 teams, Paper [here](https://proceedings.mlr.press/v212/wang23a.html)
- *2022.6* 11th of the MIDOG2022 challenge of MICCAI2023, Paper [here](https://www.sciencedirect.com/science/article/pii/S136184152400080X)


# 📖 Educations
- *2024.08 - 2026.06 (now)*, M.Eng. Student (Master by Research) at Department of Electrical and Coomputer Engineering, National University of Singapore, supervised by Prof. Yueming Jin. 
- *2020.09 - 2024.07*, B.Eng. Student at Department of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), supervised by Prof. Yongbing Zhang.

<!-- # 💬 Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

# 💻 Internships
- *2021.10 - 2024.06*, Research Assistant in Harbin Institute of Technology (Shenzhen) and Tsinghua Shenzhen International Graduate School, supervised by Prof. Yongbing Zhang.